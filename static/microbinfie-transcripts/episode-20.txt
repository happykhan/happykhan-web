Hello, and thank you for listening to the MicroBinfeed podcast. Here we will be
discussing topics in microbial bioinformatics. We hope that we can give you some
insights, tips, and tricks along the way. There's so much information we all
know from working in the field, but nobody writes it down. There is no manual,
and it's assumed you'll pick it up. We hope to fill in a few of these gaps. My
co-hosts are Dr. Nabil Ali Khan and Dr. Andrew Page. I am Dr. Lee Katz. Andrew
and Nabil work in the Quadram Institute in Norwich, UK, where they work on
microbes in food and the impact on human health. I work at Centers for Disease
Control and Prevention and am an adjunct member at the University of Georgia in
the U.S. Hi, so this episode on MicroBinfeed, we're continuing our discussion on
genome assembly, and we're going to be discussing methods on making the most of
your data. So let's get right into it. What are some of the steps you've got
available for basic preprocessing of sequence data before we start our genome
assembly? I've used Trimomatic for trimming reads as my very first step, for
Illumina reads. And I know for PacBio, Gene Myers came out with Das Scrubber. I
think he's got like the Dazzler suite. It's kind of cool. Like he goes through
the data and looks for cameras and, you know, really obvious errors and then
fixes them quite well, as well as trimming everything. And of course, you know,
once your data is slightly better, then you get a better assembly at the other
end, unless you don't have enough coverage, in which case it can actually make
things worse. So it's kind of a, it can be a double-edged sword. With Illumina
data, obviously the way the technology works, you could just kind of chop the
first few reads off, or first few bases off, and then chop the last few bases
off, and then, you know, more or less that's going to be how good you can get
the data. But some people do more, I suppose, intelligent trimming. So you know,
you look for adapters, and you'll look for low-quality bases in the middle of
the experiment. But often that indicates that there's a fundamental error with
the way the instrument has been run. You know, maybe it ran out of reagents, or
the machine is paused and people are fiddling around with it, then they restart,
and then, you know, the quality can drop off a cliff while it restarts. What
about you, Lee? For short-read trimming, Trim-O-Matic is wonderful. I actually
have my own house tool I use sometimes, but I think that Trim-O-Matic still
outcompetes it. Yeah, so I use Filt-Long from Ryan Wick, and that, you know,
filters out your low-quality reads and your kind of low-quality stuff. It's
quite good. And I use Portchop as well for removing adapters, and it can chop up
chimeras, so if there's an adapter in the middle, then obviously, you know, that
shouldn't happen. But I know Ryan is no longer supporting that. Right. Right. So
for me, in terms of read trimming, I use, I like FastP, it does everything out
of the box. Trim-O-Matic as well is a favorite. And then for long reads, so
particularly for PacBio, the in-house PacBio software for the CCS and its own
cleanup tools are actually pretty decent for looking for chimeras and all sorts
of weird nonsense like that. One thing that's always bothered me is, that people
don't talk about that might actually make a big difference is, how exactly do
you trim the reads? Do you trim, do you assume, do you have a fixed quality
score, say Q30, and then you trim all the flanking bases that don't meet it, or
do you do like an average window and average out the quality, and then remove
windows that don't, that, you know, until you increase the average up to the
point where it's acceptable? Well, I suppose we always know that the first, say
at Illumina, the first, say, 15 bases are usually pretty weak, and we know that,
say, read 2, the end of the read usually falls off quite rapidly. So you don't
need to do anything fancy, I don't think. You know, you just do a bit of
chopping, there you go. So that's like blunt end trimming, but you don't do like
any more trimming on quality? I do, I do, but that's the simplest thing you can
do. A lot of times I'll use something like, do you guys use PrintSeq, or, so
there's this thing called PrintSeq, and about the same time actually, in-house I
was developing my own tool called CGPipeline, and there's a script in there
called RunAssemblyTrimClean, which is, I know, the best name ever. But it trims
based off of quality off of the edges, and usually I give it some more or less
higher or lower quality, like a 20 or a 30, to just trim until it reaches a base
with the specified quality. And then sometimes I do a rolling average, but I
don't feel like there's a huge difference between either method. And then the
same script, either PrintSeq or RunAssemblyTrimClean, will remove reads of too
small length, or reads that don't have enough average quality. Yeah, it's
interesting with reads of too short a length. The other thing is overlapping
reads as well. Sometimes for whatever reason, you've sequenced from both sides
in, and you don't actually have any template, any blank insert in the middle of
those, you sort of, the tail of one is actually the, winds up being the start of
the other. And often that can be a bit confusing for assemblers. So if one thing
that, this is implemented in Shovel, but it's a trick that people have been
using for ages, which is you merge the two together, make a singleton, and then
you add that as a singleton read into your assembly. That can improve, that can
actually have some decent improvements in your assembly quality as well, just
doing that. Do you guys do any read correction at all? So we have a paper that
we're doing with Darlene Wagner in-house. Hopefully we'll publish it this year.
We're calling it read healing. That's quite a nice term. I've never heard of it,
but it actually does kind of make sense. She splits it up into read correction,
read trimming, and read filtering. And you brought up read correction, and I
think that's like a really neat idea, but I think that people who use it have to
be very cautious. I don't know. Do you guys use read correction right now? Only
for long read sequencing. Of course, the danger there is that if you try and
correct, say, Illumina reads, you may get rid of minority variants. But I know
in the past, when I've looked at read correction, say doing de novo assemblies
in long read sequencing, you can lose plasmids. Only lunchtime today, I was
talking to someone who'd lost a 3kb plasmid in some nanopore data, and of course
it's like, well, you know, it's probably, they've taken, say, a cutoff of 5kb,
anything under 5kb has been used to correct the longer reads, and then your
plasmid has magically disappeared. So it's something to be aware of. So to
identify this, I developed TipToft, which is a software package that's freely
available, and that will look for the ink type and the rep sequences for
plasmids. So you can tell from your raw data, is there a plasmid there that
you're missing? And then you can go back and then go and find it, because if you
know it's there, then it's easier to find it, rather than just going on a blind
fishing expedition. Very cool. Yeah, so from the literature, I think if you go
back and read about read correction, for the Brin assembly, read correction
theoretically makes a lot of sense. So the idea here is that your genome graph
basically starts diverging every time you have a single base change, and so if
you have a whole bunch of reads with single errors all the way through them,
then your assembly graph is going to be incredibly huge. And when you try to
traverse it, you're probably going to make mistakes. So for that reason, read
correction makes a lot of sense, because you take out all of those errors, you
simplify your assembly graph, and you can get something. And you'll hopefully
avoid downstream errors as well. But I agree with you, Lee, that the sort of
problem of your kind of, and what Andrew brought up as well, that you're missing
minority variants. You don't get to see, you're scooping a lot of stuff under
the carpet, and you're sort of cherry picking a little bit. You're introducing,
you're sort of over-estimating the quality of your data. And that may, that also
sort of seems a bit strange as well to do that. So I sort of have mixed feelings
about read correction, but generally for Illumina data these days, your Q value
is 40, 50, whatever. It's incredible. The error that you're expecting is not
that much of a problem. And you've got the memory that you can throw it away for
large assembly graphs. So for read correction, for short reads, I don't do it
unless I have to. And long reads, yeah, you can.  of still have to do that read
correction. Do you guys ever deal with PCR duplicates or optical duplicates? In
the context of read correction also, I created this script one time to remove
identical reads, but in doing so I thought maybe identical reads could be used
to kind of say that the FRED score of the duplicated reads are higher, and so
you see the read twice, therefore there's much less chance that there was an
error in that read. It's found exactly at least twice. So I don't know, I never
put that idea past anybody, but sometimes I'll deduplicate reads, but any
duplicated read I'll give them a higher FRED score, all the nucleotides in that
read. So I know Picard can do it, it's got an option for finding these and
identifying them, which is kind of cool, because you can get incredible bias if
you haven't maybe done the wet lab side of things properly. You can have a huge
over-representation of particular sequences and then the rest are under-
represented, and that can really mess up your assembly, you know, if you have
such uneven coverage. So you really do need to be careful. Another thing I know
people have done is they will look at k-mer distributions, and then they'll get
rid of really over-abundant k-mers, because they say, well that's probably
uneven coverage, and if we even it out or we subsample, it can possibly produce
a better assembly. And saying that actually, subsampling your data, if you have
too much data, can now often give you a better assembly. You know, if you have
10,000-fold coverage, then your errors are going to start looking like real
signal, so that's going to be a problem for the assembler. It'll look like
there's minority variance in there and the graph will just go crazy, whereas if
you subsample it down to something sane, maybe 100 to 100x coverage, it'll all
assemble nice. Very cool. Yeah, I think with the over-represented sequences, I
think both of those with the k-mer abundances and the over-represented sequences
come for free as part of FastP, so I always flick through those. Ah, you're
lazy. Yeah, but our sequencing here is pretty good. I never see that many
problems. I obviously haven't seen enough sequencing then. Well, I mean, usually
if those are bad, there's a lot more other things going on that hit you before
you get to that point of the report. True. Do you remove the low k-mer coverage
k-mers too? No, I mean, maybe that is an important piece of the genome and you
don't want to lose it, but it could also be errors as well, you know, maybe one
base error or it could be minority variance. You have to be very careful
removing low k-mers. If you remove very high abundant k-mers, you also run the
risk of removing, say, high copy number plasmids, so you have to be very careful
with those. Sometimes doing a bin assembly and subsampling, you can actually
recover those then from your data. In the same vein, you can actually remove low
copy plasmids as well by getting rid of low abundant k-mers. Plasmids are not
always at chromosome or above. They can sometimes be a little lower than the
chromosome. This is a big problem with plasmid spades, actually, because I'm not
sure if you know how it works. There's now a flag in spades for plasmids and
they published a paper in its own right on this, but what they do is they will
look at the distribution of k-mers in the chromosome, or what they think is a
chromosome, and then they'll try and work out what is a plasmid, and so they
will subtract the k-mer abundance of the chromosome from the k-mer abundance of
what they think is a plasmid and then do assemblies on those and then pull those
out. But the problem there is that if your plasmid is in more or less the same
copy number as your chromosome, then it can't spot it. It can only spot it, you
know, if there's maybe a two-fold difference or a half. And when that does
happen, you know, it's very good at that, but it can get confused and it can
miss quite a bit. Yeah, people have been trying that coverage trick for ages.
There's plenty of edge cases where it just doesn't work. That's why you need to
use PlasmidTron. You want to talk about PlasmidTron for a second? Yeah,
PlasmidTron is a really cool program I wrote for short read data, and basically
you can have a set of cases and a set of controls, so maybe disease versus
healthy, and then you want to know what is in common and what's different about
those, you know, what makes the disease, you know, maybe more severe. And it'll
then do a k-mer analysis, pull out the k-mers that are maybe unique to disease
case, and then go and do an assembly of those, and then I'll try and stitch them
together. So basically the idea is you can get out the mobile genetic element
the other end that maybe is implicated in the disease, and we used this quite
successfully for a salmonella typhi outbreak of extensively drug-resistant typhi
in Pakistan. And what happened there is a plasmid, an XDR plasmid, jumped into
an MDR strain of typhi, and it was causing like pretty, you know, big problems
obviously. And using this software we could tell from the background strains
versus the outbreak strains, we could tell what the plasmid was. It came in
three different chunks, and we had more or less the entire plasmid from short-
read data, and it's only when we went and did long-read sequencing later that we
were able to say, yeah, we got it right. Very cool. And that's not the full name
of of Plasmatron, is it? No, it's officially Plasmatron 5000. I love it. Is that
a Dilbert reference? No, I think it's a Futurama reference. I think it's really
interesting and appropriate that we were talking about plasmids in the time of
read correction, because I think that you do have to consider plasmids when
you're read correcting before you lose your plasmid. That was really good. So
that's all the time we have for this session. I've learned a lot going back over
all the tips and tricks for pre-processing your read data before de novo genome
assembly. We were introduced to the delightful term of read healing, and I'm
sure that we'll continue this discussion of genome assembly another time on the
MicroBinV podcast. Thank you all so much for listening to us at home. If you
like this podcast, please subscribe and like us on iTunes, Spotify, SoundCloud,
or the platform of your choice. And if you don't like this podcast, please don't
do anything. This podcast was recorded by the Microbial Bioinformatics Group and
edited by Nick Waters. The opinions expressed here are our own and do not
necessarily reflect the views of CDC or the Quadrant Institute.