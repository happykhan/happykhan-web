Hello, and thank you for listening to the MicroBinfeed podcast. Here, we will be
discussing topics in microbial bioinformatics. We hope that we can give you some
insights, tips and tricks along the way. There's so much information we all know
from working in the field, but nobody really writes it down. There's no manual
and it is assumed you'll pick it up. We hope to fill in a few of these gaps. My
co-hosts are Dr. Nabeel Ali Khan of Enterobase, Grape Tree and Brake Fame, and
Dr. Andrew Page of such works as Plasmatron 5000, Rory and Gubbins. Hello. I am
Dr. Ali Khan, and you might know me from my tree-making pipeline, Mass Tree, or
my SNP pipeline, Liveset. Both Nabeel and Andrew work at the Quadram Institute
in Norwich, UK, where we work on microbes in food and the impact on human
health. I work at the Centers for Disease Control and Prevention and am an
adjunct professor at the University of Georgia in the U.S. Today on MicroBinFee,
we'll be talking about the ubiquitous SAM format. We use it every day, but how
did it come into being? And how do you explain all of its idiosyncrasies?
Nabeel, why don't you start us off? Yes, I think the first place to start is to
actually explain what the SAM format actually is. So the SAM format was set up
by Heng Li, who was working for Richard Durbin at Sanger, and mostly pulled it
together over a couple of weeks, and it hasn't changed much since then. And this
was back in 2000, and it was first published back in 2009, and it was first seen
in the wild with BWA, which is a short read aligner. The specification and
implementation continues to be managed and developed by Sanger and EBI, and what
it's actually trying to capture is it's measuring how reads align to a
reference. And it adds metadata, quality information. It's tab-delimited and
fairly easy to read. And it comes in three different flavors. So you can have
SAM, which is a plain text representation. You can have BAM, which is a binary
representation. And you can have CRAM, which is a compressed or potentially
lossy archive format. But I think Andrew's probably the best one to tell us how
it actually came into being. Well, I suppose I was at Ground Zero back in the
day in Sanger when a lot of this was being developed. And I was in a group
called Sequencing Informatics, and at the moment they're the kind of group that
is currently continuing to develop CRAM and SAM, which is fantastic. We really
do need this kind of institutional long-term support, and it's all driven
overall by GA4GH. So it came into being because before short reads came about,
the data volumes were really, really low. And for the most part, these things
were locked away in these bespoke databases. All the data was there, and the
teams of developers lovingly hand-curating everything. And if you remember Willy
Wonka and the Chocolate Factory, someone built a chocolate palace. Well, this is
what genomics was like back in the day. It was a chocolate palace. But of
course, then along comes Illumina with their 37 base single-ended reads, and it
just blew everything out of the water. And the chocolate palace came down
because no one could actually keep up with all of the data coming through. And
so it meant that people had to go and actually start making these short read
aligners in formats that could actually, file formats, that could actually be
used and reused quite easily and could cope with all the extra information and
data that is out there. But Lee, before, so do you, I mean, Lee, do you remember
before we had such formats like the SAM format, what was there for read
aligners? Yeah, we used to, so even way back in the day with Sanger, we had
basically, Fred and Frapp to align things with each other. And we used CONSED as
kind of the thing that we used to just like manually put reads together. And it
came out, it came, it exported kind of this blast crunch format. It was very
clunky, not very parsable. I remember getting Fred or Frapp to install was sort
of a rite of passage. Yeah, you got like an email saying like, here's the
package, please don't share it. Do you remember that? Yeah, I remember that. And
it took, and all of the folders were sort of hard-coded for each project. So it
took you about a day to set up, set, set it up. Thank God we've moved on from
that. I still have the email though, just in case I ever need to install it
again. Yeah, I think I've got a copy somewhere on an external drive. But then
there were other formats like ACE format. I mean, that wasn't, well, that was a
little better. Yeah, it actually was parsable. And it's very clunky. And I'm
glad we don't have to use it again. I was actually digging up an old post on
Biostars where I was, it's from nine years ago now, where I was trying to figure
out, how do you parse this thing? It was awful. And what was ACE format used
for? It was used for 454 mainly. I think that there was an ACE format predating
it, but 454 kind of co-opted it and came up with their own variant of it. I
think the variant was like different coordinates. Yeah, it was the primary
output from Noobler, which was 454's assembler. But we don't have to worry about
454 Noobler anymore because 454 is dead. No, you can't say that. I don't want to
be controversial, but yes, it's dead. But I recall that around 2008, like there
seemed to be one new assembler or one new read aligner coming out every week.
But actually I look back and it was only one new aligner every month for three
years. That was fantastic as a PhD student at the time, trying to keep on top of
the literature for your dissertation, just like which aligners and what does
what. Yeah, exactly. I think I came across like five different mappers that I
was trying out, but I can't remember what they were anymore. Do you remember
like any of the weird ones that you tried out during your thesis? Like
Novoalign? Yes. Saha was one. Oh, we used to use Saha too for ages. And then you
obviously had, was Bowtie? No, Bowtie was a bit later, I think. Yeah, and
basically I think most of the world uses Bowtie and BWA now, or Bowtie 2 and
BWA. But now we've gone up to bigger and better, I guess, with long reads. Yeah,
using Minimap, yeah, Minimap for the long reads. Oh, and there were also a lot
of local alignment for short reads. What was the Sanger one? Smalt is a local...
Smalt is basically Saha 2, Saha 3. And there was one I used to use called
Stampy, which was more on the... Remember that one? Yeah, definitely. People
were messing around with that a lot. I never got into that one, but everyone was
using it. Well, that was different to the BWA and the Bowtie because that was
actually a local alignment one. So in principle, it's like, yes, it takes
longer, but you should get more sensitivity with your alignment. In theory.
Although I did notice that Minimap 2 will output path format by default, which
is just another invention of Heng Li's. Oh, I don't know that format. PAF? Yeah.
Okay. Yeah, no, he defaults to something that's not SAM, so fair play to him. So
speaking of getting back to the SAM format... Sorry. No worries. There's a
number of different tools that can read it, and I think that's why it's been so
successful. So the major one, obviously, is SAMtools, and that's the working
reference. If your file doesn't go through SAMtools, well, it's not legitimate,
really. And then there are other packages such as Picard, which is from the
Broad, which is sort of paired with GATK, and there's CramTools from EBI, and
CramJS from Gmod? Yeah, I just saw that. I mean, I've never heard of anyone
using it, but I suppose it is useful if you're doing some kind of browser. Okay.
Or maybe Nodes.js or something like that. Yeah, I mean, if you want to do a BAM
view in the browser on JBrowser or Gmod, then it's pretty good. Of course,
they're all different languages, so SAMtools is C, and then CramTools was Java,
Picard, GATK would be Java, and obviously CramJS is JavaScript, and of course
JavaScript is different to Java. Never forget. Never forget. Even the internal
processing of SAM spawned out from HSTSLib as a C library to make it easier to
have SAM processing and other tools. And did you work with the creators of
those? Yeah, there was a guy called John Marshall who did the very heavy lifting
to extract all of the SAM and Cram and BAM processing from inside SAMtools and
put it out in a separate package. It just made it easier. You could include it
in other libraries then and make better software without having to reinvent
everything, and it meant compatibility was a lot easier if you fixed...  the
underlying library once, then you fix it in lots of different implementations.
And then someone else called Peter Danicek was also heavily involved in that,
and he was extracting out the BCF tools bit into separate packages to make it
just a bit easier, you know, because CodeBase had gotten massive at that point,
after many years of development. And then that, is that the basis for certain
other modules and other languages like PySAM and things like that? Yeah, they
can just include it. And now it's been taken, development has been taken over by
James Bonfield in the Sanger Institute. He's been there for years, and he was
involved in like many different projects like, I think it's called Stadin and
GAP5 and a load of these other things. So you know, heavy duty, long term
support there. I remember those guys, yeah. Well, let's go around the table. And
because SAM tools is obviously very popular, there's no denying that. But what
is actually good about it? What do you, what do you guys think is key to its
success? Well, if I, I don't think I have any inside knowledge on it, but like,
just from my point of view, like a real key thing about the SAM format was, it
kind of made this this idea of a stopping point. Like you are in your project,
and you want to find out snip calls. And I think back when I was naively
thinking about this in grad school, like I just wanted to find the variants with
no middle step. Like there's no step one, step two, step three, it was like, get
all my reads and find my snips or find my X. And I felt like looking at the SAM
format was kind of revolutionary. Like, let's stop there. Let's have all the
reads there. And then we can analyze from there. Yeah, I think one of the things
that I found, there were a couple, there were a bunch of different things for me
that made it, that I liked, or still like about it. The first thing is that once
you start moving into the BAM format, you can, you can stream, you can stream
all of the operations on your alignment. You don't have to put in this massive
five gig file into memory and work with it. You can just immediately jump to
certain positions and just extract the relevant parts out of that. And it's also
really easy. Once you get your head around the syntax, it's actually really easy
to do the piping in SAMtools. So SAMtools view to SAMtools sort to BCF tools.
And that's just one line. And that was actually really simple, as opposed to
having to pipe it into one format and change it with a script and then to
something else and to something else. And so I found that incredibly useful.
Well, personally, I really like the bitwise flags. So you can just say, give me
all the reads that didn't map or all the unpaired reads, or maybe get rid of all
the reads where you've secondary alignments, that kind of thing. So it's very
easy just to pull out whatever you want. And it's quite quick as well. Yeah. And
there's a lot of information embedded in it as well, like along with the reads
and their coordinates, whether you've also got the mapping quality there, and
you've got information about how well it's aligned to the cigar string, which
doesn't have anything about the snips. It's just got sort of how that block is
aligned to the reference, so it's information about clipping or indels and
things like that. But any secret hidden features from you guys that people
should know about? I like the fact that PacBio have modified the BAM format so
that you can have methylation information in there as well. No modify, but
they've added in an extra bit, which is quite nice because it's standard format,
you have standard APIs, and then you can get out this extra bit of information
that is quite useful with this new sequencing technology. And that's put in on a
base by base level in the pileup or on the reads? It's on the reads. So one of
the best hidden features that I like is called SAMTools Stats, and you can use
it with Plot BAM checks. So it will give you lots of information about the
reads, and it'll give you distributions about the insert size and different
biases. And it's really, really useful, particularly when you start plotting all
of this with Plot BAM check. It gives you pretty pictures, and it's just a
really, really fast way to get an idea of what is actually in your file and how
it looks. Is this one of the tools that print your pileup in ASCII? No, no. So
what it does is it does an analysis and outputs it into a huge, big file that
you can't really read. If you do head on it, it actually gives you useful
information, the first 30 lines do things like it tells you the number of reads
that are mapped, the mapping qualities on average, that kind of thing. So some
basic stats, which is really super useful. But then the bottom part of it is
basically just the input into graphs in, I think, SCANOO And that's what Plot
BAM check then takes and interprets and makes pretty pictures. Okay. Well, this
isn't really a hidden feature, but this is something some people might miss, is
you can just grab a subset of a BAM file by specifying the name of the
chromosome or name of the sequence, the start and the stop. And that allows you
to take a region of interest and just clip that straight out and just work with
that subset of the reads that map to that region. And that is much easier to
work with than having to pass the mapping across an entire chromosome. But
that's not really a hidden feature. That's in the manual. You just have to read
it. No, that's really cool. Because I mean, like along those lines, it's, it's a
highly parsable format and you can pipe it. So it's like you, you want to
analyze just like one section of genome. I love it. Just like you were saying,
like you pick your coordinates, it'll jump right to that spot on the file and
give it to you very quickly. That's really awesome. There's one sort of concept
in the SAM format that's a little confusing, which is the soft clipping and hard
clipping and what that actually means for reads. And what it essentially boils
down to is it's trying to capture situations where the entire read doesn't
necessarily map. So part of the read or the middle of the read is mapped to your
reference. And then there's this bit hanging off the end that doesn't, that
doesn't align. And it, I think it's up to the discretion of the mapper, but
sometimes they will soft clip it, which means that the, those bases that don't
align on part of the alignment, but it's actually retained in the read sequence
in the BAM file. And other times they can be hard clipped where they are not
part of the alignments and they've been removed from the read sequence as well.
So it's sort of, the tool itself has decided that this is totally irrelevant and
it's just taking it out to simplify your, your result for you. What is the
advantage of soft clipping versus hard clipping though? Well, your file is
smaller if it's hard clipped. That's what I would think. I think I've done that
before just to make sure I get small file sizes, just to be like a good citizen
on my hard drive. Often often the hard clipping is when it's like hundreds of
bases that are hanging off the edge that really don't make any sense. And
they're just hard clipped off. So yeah, I've seen a lot in long reads when even
that poor sequencing. Yeah, in those cases, like 3000 bases are hard clipped
off. Well, chimeras or whatever. And yeah, I mean, actually what's actually
happening is, for me in the wild, when this actually happens is it's usually
indicative that it's sort of, you've mapped to a duplicate region or collapsed
repeat or something like that. And you've actually got your chimeric reads that
are, that would be different, that it's a duplicated region, but the flanking
region is different. But they sort of get collapsed into one area. So that's
generally why, to me, that's the major reason why you'd see this sort of
clipping issue. I don't know if there's any other instances where you'd see
clipping, no? Right. Well, maybe if it's split over two different chromosomes,
you can see that sometimes. Yeah. Oh, yes. Yeah. You'd see that as well. If it's
all different contigs or different chromosomes. Yeah. That's cool. So to
visualize these things, I don't know, I think early on in grad school, I started
off with Artemis. And did that, I'm not even sure where that came out of, was
that from Sanger? Yes. I worked in the group that developed this, and it was Tim
Carver. Tim Carver was maintaining for many years and developing it, and he
developed BamView. And then BamView got integrated into Artemis. So Artemis has
really good support for Bam and Cram, and I think it was one of the first
viewers, it was the first viewer that actually incorporated Cram support very,
very early on. And yeah, you can just use it in, you can pile up all your reads
on top of your reference genome or your annotated reference genome, and it's
quite handy. I use it all the time. It just works. I used it for things that
were not for mapping reads, but I had a lot of exposure to it just for looking
at homology of genomes. It is really cool. There are a lot of hidden features
that I just have never used on it. So thank you to you and Sanger. Last on me is
Tim Carver. Thank you, Tim Carver. I think later on, I started using the
TextViewer because I just, I...  I am a fan of using ubiquitous software. Like
I'm that guy who uses XRX instead of Parallel. Cause I know that Parallel, even
though Parallel is a lot more feature rich, I know that XRX is on any computer
I'm gonna go to. So just like that, I, just along the same lines, I use SamTools
TextViewer and it's extremely, I don't know, it's not feature rich, but it gets
the job done and it's awesome. So, I mean, I don't know how else to put it. I
just use it all the time. It's also very, very fast. Yeah, it's really lean and
very fast. I agree with you. So it just pops up right away as soon as you want
to use it. And if you know, just like earlier, like if you know like the region
you want to go to, just like also tell it the region you want to start off on
too. It gets right to the point. I also use, right after that, I remember I
actually went on to tablet, but soon after that, I actually, I didn't use tablet
for too long. Like it's really good, it's lean, but for the cost of like the
computing cost, I actually thought that IGV from the Bird Institute was a really
good deal. It has basically everything I want in a read viewer, just you can
visualize the reads, you can make regions, you can import regions like genes and
overlay them. You can overlay like five different genomes at once and see how
they all align. It's really cool. I really liked the track system in IGV. At the
time when it came out, that was probably the only tool if you wanted to look at
10 or 20 different genomes mapped to a single reference at once, that was really
the only way you could do it, I think. Yeah. With it making sense. No,
absolutely, you're right. I know it wasn't just this last year, but then I
became aware of ASCII genome also this last year. But when I look up the actual
article, it was in 2017. So that's been under the radar for a while. And I think
it's a really good drop-in replacement for the samples text viewer. Again, I
have my bias just because I want my software to be on every computer I go to,
but it looks really good. So I just want to talk for a minute about CRAM and
HTS-Lib because this is really the future of the SAM format. So CRAM is like a
compressed format of SAM and it's compressed and it's binary. One major issue is
that data volumes are going up exponentially, but the costs of hard disks are
going down very slowly at about, I think, 20% a year. And in some years, it's
slightly increased as well when there have been natural disasters. But CRAM
overcomes it by allowing you to actually throw away a little bit of your data.
And it was developed originally with Vadam in EBI and James Bonfield in the
Sanger Institute. But EBI are like the overall drivers of the specification.
It's now been taken over by GA4GH, the Global Alliance for Genomics and Health,
who are, I suppose, the overarching working group and committee that make sure
it stays in the pristine format that it is. But anyway, the need for better
compression came about because BAM was just a way of compressing binary data,
you know, taking your ASCII, turning it into binary, and then there you go. And
it was grand, but if you think about what you need now, if you have 200x
coverage of a bacteria and 200 of those reads match the reference, you don't
really need to keep all 200 reads. You can just say, this is the same as the
reference, and you can throw those away. But CRAM lets you tune that, so you can
say how much data you want to keep and how much you want to throw away. And
maybe you only want to keep the SNPs to the human reference genome or whatever.
But most people use it with a lossless compression, so you don't throw anything
away. In 2011, there's this competition called Sequence Squeeze to see who could
get the best compression of short-read data, and it was like an international,
very highly competitive competition with people from Russia and the US and the
UK. And James Bonfield won that overall from his windowless basement. They
literally stuck all the developers in the basement at the end of the car park.
Nice offices, though. And then this fed into CRAM. Eventually, he started off, I
think, with Scramble, it was called. But under the hood, basically, CRAM takes
these blocks of reads, it reorganizes them slightly into a more compressible
format and then it compresses those at GZIP, I think, and then it chains those
together. So that means you still mostly have random access as long as you don't
mind uncompressing a block at a time. And you get much better compression
because maybe if you keep all the reads together, you can compress those because
they're mostly the same letters. And if you keep the quality scores together,
you can compress those better because they're all physically together and they
share the same stuff. And you can get like 40% or 50% space saving just by using
CRAM over BAM, which is just insane. And I should point out that nobody should
be using SAM these days on its own because those files are just insanely big.
There's no need for it when you have two other formats that are much smaller.
Why were they in the basement? The windowless basement? Yeah, in Sanger. Well,
in Sanger, they decided to put all of the sequencing informatics and developers
and people supporting them into a refurbished basement. And I lie, there's a big
open plan office which has some windows at a kind of head height. So you do get
a bit of natural light. But I think James, I recall, was in an office that was
kind of an internal office to that even. And so there was no natural light at
all. But they were nice offices. They were brand newly refurbished with
everything you could possibly hope for. Apart from when it floods, there are
flood barriers at all the doors, which is what you get when you build on a flood
plain. But anyway, that's an aside, right? So the first working implementations
of CRAM were actually being done kind of collaboratively and kind of
competitively. So in EBI, which is a different building to Sanger, like
literally there's like 20 meters away from each other. And they were building
their implementation in Java and then in Sanger, they're doing it in C. And
eventually the C version just got more usage and won out. But it was good as
well by having two different implementations of one specification. It meant that
all the little issues were teased out pretty quickly. And it meant there was a
lot of scrutiny for the format, which made a very robust format that's gonna
last for many, many years. So why don't we use CRAM outright instead of BAM
right now? CRAM was developed originally as like an archive format. So for EBI,
people like that. I suppose the limitation would be that there isn't as much
support in tools for CRAM. So it's until you have that critical mass where it's
just everywhere and you can't avoid using it. And also then you need to be able
to download it from archive. So when NCBI start providing everything in CRAM,
then you're onto a winner. All right, guys, you heard it here first. Get on it,
get CRAM support into your various tools. Yeah, I need to get that in my... I
need to actually start learning how to read CRAM. Is that the internal format
that they archive in EBI? I don't know exactly what they do because they do
provide everything in fast queue format. So although they do ask you to provide
CRAM or they like to have CRAM, if you give everyone back fast queues instead of
CRAM, then that's kind of defeating purpose. But you know, there you go. There
is some limitations though with BAM and I see it coming to an end because of
long reads. So there's a problem with the cigar string where you can only have
64 kilobases. Is it kilobase or kilobytes? One of those. Anyway, there's a
limited length of a cigar string and when you have really long, noisy reads
coming from Nanopore, it just kind of breaks it because the cigar string gets
too long for BAM. And there was like a month or two many years ago where there's
this competition between the Long Read Club where they kept getting longer and
longer reads. And then the developers of SamTools going, oh, you know,
everything is breaking. So it took a little while for that to settle down. But
now CRAM works for really long reads and there's no problems anymore. And if you
are doing long reads, you have to use Sam or CRAM, but not BAM in between. So
that is a pitfall. And I've fallen into a pitfall a few times where I've been
working with long reads. I've done a mapping, converted to BAM and then went,
oh, wait, it's just broken. It's fallen over. Actually, I think I've done some
projects with Nanopore where we do many.  and we get a BAM file in the end but
it doesn't give me any warning like is it is it secretly dying well no you when
it dies you'll know it dies but you may spend half a day trying to debug it you
know why is it dying and there's one particular read that's like 75% away
through your file you know it's just one of these random things that really
drains your time and then you'll probably find that if you just use cram instead
that actually everything will probably just work good to know that's yeah that's
very good to know but all right but then where to next what's the future for sam
so it's a robust format it's well it has some very well supported implementation
despite some teething problems with long reads i think it's going to be around
for a very very long time and i think just some of the history we've been
talking about in terms of the collaborative nature of it the community
involvement is really a good example of how critical bioinformatics projects
should be run any final closing statements from you guys i feel like i learned a
lot here um bam format is just like ubiquitous it's my bread and butter for a
lot of things but i feel like my eyes have been opened that there is a new
format in the future and maybe even after that like something can always be
improved on this and that's really cool and i like the fact that uh most
bioinformatics software has a lifespan of three or four years but in this case
because it's been so well managed and developed over a long period of time and
has this international backing and committees and whatnot making sure it
continues into the future and it's all on github as well so the community can
interact and propose changes that this is a model for how we should be
developing all bioinformatics software moving forward so so as a final final
question then if we did not have the institutional backing for the sam format
would it be any way near the state it is in today highly unlikely we probably
have 20 different mapping formats that we have to convert in and out of like we
do for many other formats like annotation yeah i think we'd be stuck with ace i
think we'd be yeah well it would be every little phd project or postdoc project
would have a new format that you'd have to take care of and then each sequencing
instrument vendor would come up their own format as well so you know you'd be
back in you'd be rolling back progress all it takes is a windowless basement
thank you all so much for listening to us at home if you like this podcast
please subscribe and like us on itunes or google play and if you don't like the
podcast please don't do anything this podcast was recorded by the microbial
bioinformatics group the opinions expressed here are our own and do not
necessarily reflect the views of cdc or the quadram institute you